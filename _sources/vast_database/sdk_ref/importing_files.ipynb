{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353faac2",
   "metadata": {},
   "source": [
    "# Importing Files using API\n",
    "\n",
    "```{important}\n",
    "This notebook is in the process of being migrated to Vast Data Platform Field Docs.  It will probably not run yet.\n",
    "```\n",
    "\n",
    "```{seealso}\n",
    "The Vast DB SDK API Documentation is available [here](https://vastdb-sdk.readthedocs.io).\n",
    "```\n",
    "\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a768d",
   "metadata": {},
   "source": [
    "Bulk import of data into VAST-DB, can be done with any method supported by the analytics/query engine.\n",
    "\n",
    "Inserts, CTAS commands (CREATE TABLE AS [QUERY]), python scripts processing data frames, etc. Tools like Spark make this easy, but this type of processing will be slow compared to the LOAD operations supported by various databases. VAST-DB includes LOAD capabilities.\n",
    " \n",
    "Ingesting Serialized Data Objects (LOAD)\n",
    "Parquet objects housed on VAST can be ingested directly into a VAST-DB table using an RPC call issued via the VAST-DB APIs. Issuing these calls is supported from Trino and Spark and this will usually be how these calls are made. Import process will look like this:\n",
    " \n",
    "- The target table with schema is prepared in VAST-DB\n",
    "- Data is placed in an S3 bucket on VAST storage\n",
    "- An RPC call is made to a VAST-DB CNode directing it to the object location(s) in VAST\n",
    "- The VAST-DB CNode cluster will then divide the workload and import the data directly from the file and into the prepared table. \n",
    "\n",
    "Currently, Parquet is the only format supported for this operation, however creating additional filters for ORC, delimited text, CSV, JSON, etc. is a simple development task (if you want it, just ask).\n",
    "Bulk importation of data into VAST-DB, can be done with any method supported by the analytics/query engine. Inserts, CTAS commands (CREATE TABLE AS [QUERY]), python scripts processing data frames, etc. Tools like Spark make this easy, but this type of processing will be slow compared to the LOAD operations supported by various databases. VAST-DB includes LOAD capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "- **Method**: table.import_file\n",
    "- **Usage**: Imports a list of Parquet files into this table. The files must be on the VAST S3 server and be accessible using current credentials.\n",
    "- **Args**:\n",
    "  - `files_to_import` (Iterable[str]): An iterable of file paths to import.\n",
    "  - `config` (Optional[ImportConfig], optional): Configuration for the import operation. Defaults to None.\n",
    "- **Raises**:\n",
    "  - `errors.NotSupportedCommand`: If the operation is not supported on the current table.\n",
    "\n",
    "---\n",
    "\n",
    "- **Method**: table.import_partitioned_files\n",
    "- **Usage**: Imports a list of Parquet files into this table, with each file having its own partition values. The files must be on the VAST S3 server and be accessible using current credentials. Each file must have its own partition values defined as an Arrow RecordBatch.\n",
    "- **Args**:\n",
    "  - `files_and_partitions` (Dict[str, pa.RecordBatch]): A dictionary mapping file paths to their corresponding partition values.\n",
    "  - `config` (Optional[ImportConfig], optional): Configuration for the import operation. Defaults to None.\n",
    "- **Raises**:\n",
    "  - `errors.NotSupportedCommand`: If the operation is not supported on the current table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9357dc7a-1add-4796-8e59-f898bf6aed90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T16:43:12.657340Z",
     "iopub.status.busy": "2024-01-08T16:43:12.657010Z",
     "iopub.status.idle": "2024-01-08T16:43:12.664384Z",
     "shell.execute_reply": "2024-01-08T16:43:12.663137Z",
     "shell.execute_reply.started": "2024-01-08T16:43:12.657316Z"
    },
    "tags": []
   },
   "source": [
    "## Create a database session\n",
    "\n",
    "This section creates a session as detailed [here](./create_session.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d604c-205e-4e2d-a981-97c3e0e2bc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet vastdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632935e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change these variables to reflect your environment, E.g. \n",
    "#\n",
    "# ENDPOINT = 'http://your_vast_endpoint:12345'\n",
    "# DATABASE_NAME = ...\n",
    "\n",
    "ENDPOINT = os.environ['ENDPOINT']\n",
    "ACCESS_KEY = os.environ['ACCESS_KEY']\n",
    "SECRET_KEY = os.environ['SECRET_KEY']\n",
    "\n",
    "DATABASE_NAME = os.environ['DATABASE_NAME']\n",
    "\n",
    "# Schema and Table will get created if they doesn't exist \n",
    "DATABASE_SCHEMA = os.environ['DATABASE_SCHEMA']\n",
    "TABLE_NAME = os.environ['TABLE_NAME']\n",
    "\n",
    "# S3 File Details\n",
    "BUCKET_NAME = os.environ['BUCKET_NAME']\n",
    "LOCAL_FILE_PATH = 'flights.parquet'\n",
    "S3_FILE_KEY = 'pythonsdk/import/flights.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838bdd64",
   "metadata": {},
   "source": [
    "The following cell contains code you will need to run to establish a session.  \n",
    "\n",
    "The cell is hidden because creating a connection is not the key focus of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a3c320-a409-403e-87b1-215bb15839ff",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{ENDPOINT=} \n",
    "{ACCESS_KEY=}\n",
    "---\n",
    "{DATABASE_NAME=}\n",
    "{DATABASE_SCHEMA=}\n",
    "{TABLE_NAME=}\n",
    "---\n",
    "{BUCKET_NAME=}\n",
    "{LOCAL_FILE_PATH=}\n",
    "{S3_FILE_KEY=}\n",
    "\"\"\")\n",
    "\n",
    "import vastdb\n",
    "\n",
    "session = vastdb.connect(\n",
    "    endpoint=ENDPOINT,\n",
    "    access=ACCESS_KEY,\n",
    "    secret=SECRET_KEY)\n",
    "\n",
    "print(\"Vast Cluster version: \", session.api.vast_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02680e90-6d48-43de-a95d-2d1c816e845a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T16:31:14.236103Z",
     "iopub.status.busy": "2024-01-08T16:31:14.234737Z",
     "iopub.status.idle": "2024-01-08T16:31:14.247483Z",
     "shell.execute_reply": "2024-01-08T16:31:14.245958Z",
     "shell.execute_reply.started": "2024-01-08T16:31:14.236034Z"
    },
    "tags": []
   },
   "source": [
    "## Import using the importer on Cnodes\n",
    "\n",
    "The importer expects that S3 files are available in an S3 Bucket. \n",
    "For the tutorial we will upload an example airline parquet file to a VAST S3 Bucket  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d42ee6-48c2-4add-9196-07a6091f9a09",
   "metadata": {},
   "source": [
    "### Import our flight data parquet file into s3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates some utility functions, `upload_to_s3` and `list_objects_in_bucket`.  The cell is hidden because it is boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "130a3310-3e64-4d22-8e4d-7051d8c7bba5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "def upload_to_s3(local_file, bucket_name, s3_file, aws_access_key_id, aws_secret_access_key, s3_endpoint):\n",
    "    # Create an S3 client with custom endpoint\n",
    "    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, endpoint_url=s3_endpoint)\n",
    "\n",
    "    try:\n",
    "        # Upload the file\n",
    "        s3.upload_file(local_file, bucket_name, s3_file)\n",
    "        print(f\"File {local_file} uploaded to {bucket_name}/{s3_file} successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {local_file} was not found.\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available.\")\n",
    "        \n",
    "def list_objects_in_bucket(bucket_name, aws_access_key_id, aws_secret_access_key, s3_endpoint, prefix=None):\n",
    "    # Create an S3 client with custom endpoint\n",
    "    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, endpoint_url=s3_endpoint)\n",
    "\n",
    "    try:\n",
    "        # List objects in the bucket with optional prefix\n",
    "        kwargs = {'Bucket': bucket_name}\n",
    "        if prefix:\n",
    "            kwargs['Prefix'] = prefix\n",
    "\n",
    "        response = s3.list_objects_v2(**kwargs)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            print(f\"Objects in bucket {bucket_name} with prefix '{prefix}':\")\n",
    "            for obj in response['Contents']:\n",
    "                print(f\">>> obj['Key']\")\n",
    "        else:\n",
    "            print(f\"No objects found in bucket {bucket_name} with prefix '{prefix}'.\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf974e82-307c-44e3-93e8-00a8e1c977cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(\n",
    "  LOCAL_FILE_PATH, \n",
    "  BUCKET_NAME, \n",
    "  S3_FILE_KEY, \n",
    "  ACCESS_KEY, \n",
    "  SECRET_KEY, \n",
    "  ENDPOINT)\n",
    "\n",
    "list_objects_in_bucket(\n",
    "  BUCKET_NAME, \n",
    "  ACCESS_KEY, \n",
    "  SECRET_KEY, \n",
    "  ENDPOINT, \n",
    "  S3_FILE_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c284d-5b19-4458-acf0-f71ce6794bfc",
   "metadata": {},
   "source": [
    "## Create Schema and use the API to start the import on the Cnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc362fcf-ccd3-4555-bb1a-1328d01ad1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema(name='python-sdk-schema', bucket=Bucket(name='demo-database', tx=Transaction(id=0x0000300000000033)))\n",
      "Table created: flights_cnode_importer\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "from vastdb.errors import TableExists\n",
    "\n",
    "# create table schema before using the importer API\n",
    "\n",
    "ARROW_SCHEMA = pa.schema([\n",
    "    ('FL_DATE', pa.date32()), \n",
    "    ('DEP_DELAY', pa.int16()),\n",
    "    ('ARR_DELAY', pa.int16()),\n",
    "    ('AIR_TIME', pa.int16()),\n",
    "    ('DISTANCE', pa.int16()),\n",
    "    ('DEP_TIME', pa.float32()),\n",
    "    ('ARR_TIME', pa.float32())\n",
    "])\n",
    "\n",
    "with session.transaction() as tx:\n",
    "    bucket = tx.bucket(DATABASE_NAME)\n",
    "\n",
    "    # first retrieve the schema\n",
    "    try:\n",
    "        schema = bucket.schema(name=DATABASE_SCHEMA, fail_if_missing=False)\n",
    "        print(schema)\n",
    "    except Exception as e:\n",
    "        print(\"Schema doesn't exist:\", e)\n",
    "\n",
    "    if schema:\n",
    "        try:\n",
    "            table = schema.create_table(table_name=TABLE_NAME, columns=ARROW_SCHEMA)\n",
    "            print(f\"Table created: {table.name}\")\n",
    "        except TableExists as e:\n",
    "            print(\"Couldn't create table because it already exists:\", e)\n",
    "        except Exception as e:\n",
    "            print(\"Couldn't create table:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89570deb-2107-4c1f-b0aa-d01e26748948",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing ['/demo-database/pythonsdk/import/flights.parquet']\n",
      "Schema(name='python-sdk-schema', bucket=Bucket(name='demo-database', tx=Transaction(id=0x0000300000000034)))\n"
     ]
    }
   ],
   "source": [
    "FILES_TO_IMPORT = [f'/{BUCKET_NAME}/{S3_FILE_KEY}']\n",
    "print(f'Importing {FILES_TO_IMPORT}')\n",
    "\n",
    "with session.transaction() as tx:\n",
    "    bucket = tx.bucket(DATABASE_NAME)\n",
    "\n",
    "    # first retrieve the schema\n",
    "    try:\n",
    "        schema = bucket.schema(name=DATABASE_SCHEMA, fail_if_missing=False)\n",
    "        table = schema.table(TABLE_NAME)\n",
    "        print(schema)\n",
    "    except Exception as e:\n",
    "        print(\"Schema doesn't exist:\", e)\n",
    "\n",
    "    if table:\n",
    "        try:\n",
    "            table.import_files(files_to_import=FILES_TO_IMPORT)\n",
    "        except Exception as e:\n",
    "            import sys, traceback\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "            print(\"Couldn't import files:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3192c0-0669-456c-9506-278ee278f6f4",
   "metadata": {},
   "source": [
    "Let's verify the import..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28fbcb89-aa8c-4c00-9bd2-1eaf0a6bdaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing rows in flights_cnode_importer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>ARR_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>350</td>\n",
       "      <td>2475</td>\n",
       "      <td>9.083333</td>\n",
       "      <td>12.483334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-02</td>\n",
       "      <td>167</td>\n",
       "      <td>216</td>\n",
       "      <td>343</td>\n",
       "      <td>2475</td>\n",
       "      <td>11.783334</td>\n",
       "      <td>15.766666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>-7</td>\n",
       "      <td>-2</td>\n",
       "      <td>344</td>\n",
       "      <td>2475</td>\n",
       "      <td>8.883333</td>\n",
       "      <td>12.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-04</td>\n",
       "      <td>-5</td>\n",
       "      <td>-13</td>\n",
       "      <td>331</td>\n",
       "      <td>2475</td>\n",
       "      <td>8.916667</td>\n",
       "      <td>11.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>-3</td>\n",
       "      <td>-17</td>\n",
       "      <td>321</td>\n",
       "      <td>2475</td>\n",
       "      <td>8.950000</td>\n",
       "      <td>11.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>2006-01-19</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>244</td>\n",
       "      <td>1781</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>2006-01-20</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>240</td>\n",
       "      <td>1781</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>17.483334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>2006-01-21</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>241</td>\n",
       "      <td>1781</td>\n",
       "      <td>15.066667</td>\n",
       "      <td>17.483334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>2006-01-22</td>\n",
       "      <td>-2</td>\n",
       "      <td>8</td>\n",
       "      <td>242</td>\n",
       "      <td>1781</td>\n",
       "      <td>14.883333</td>\n",
       "      <td>17.416666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>2006-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>-12</td>\n",
       "      <td>232</td>\n",
       "      <td>1781</td>\n",
       "      <td>14.933333</td>\n",
       "      <td>17.083334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           FL_DATE  DEP_DELAY  ARR_DELAY  AIR_TIME  DISTANCE   DEP_TIME  \\\n",
       "0       2006-01-01          5         19       350      2475   9.083333   \n",
       "1       2006-01-02        167        216       343      2475  11.783334   \n",
       "2       2006-01-03         -7         -2       344      2475   8.883333   \n",
       "3       2006-01-04         -5        -13       331      2475   8.916667   \n",
       "4       2006-01-05         -3        -17       321      2475   8.950000   \n",
       "...            ...        ...        ...       ...       ...        ...   \n",
       "999995  2006-01-19          5          4       244      1781  15.000000   \n",
       "999996  2006-01-20         14         12       240      1781  15.150000   \n",
       "999997  2006-01-21          9         12       241      1781  15.066667   \n",
       "999998  2006-01-22         -2          8       242      1781  14.883333   \n",
       "999999  2006-01-23          1        -12       232      1781  14.933333   \n",
       "\n",
       "         ARR_TIME  \n",
       "0       12.483334  \n",
       "1       15.766666  \n",
       "2       12.133333  \n",
       "3       11.950000  \n",
       "4       11.883333  \n",
       "...           ...  \n",
       "999995  17.350000  \n",
       "999996  17.483334  \n",
       "999997  17.483334  \n",
       "999998  17.416666  \n",
       "999999  17.083334  \n",
       "\n",
       "[1000000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with session.transaction() as tx:\n",
    "    try:\n",
    "        schema = tx.bucket(DATABASE_NAME).schema(name=DATABASE_SCHEMA, fail_if_missing=False)\n",
    "        if schema:\n",
    "            try:\n",
    "                table = schema.table(name=TABLE_NAME)\n",
    "                reader = table.select()\n",
    "                pyarrow_table = pa.Table.from_batches(reader)\n",
    "                df = pyarrow_table.to_pandas()\n",
    "                print(f\"Listing rows in {TABLE_NAME}\")\n",
    "                display(df)\n",
    "            except Exception as e:\n",
    "                print(\"Couldn't select data:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"Schema doesn't exist:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
