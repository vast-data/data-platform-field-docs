{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8886d85-629a-4aea-8276-ea017bb54456",
   "metadata": {},
   "source": [
    "# Spark Query Time Merge (id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b44a8-4489-4e4f-80ad-282ead4bf38f",
   "metadata": {},
   "source": [
    "## Handling Duplicates and Latest Updates with Apache Spark\n",
    "\n",
    "```{seealso}\n",
    "[Query Merge vs Insert Merge](./query_time_merge.md)\n",
    "```\n",
    "\n",
    "In this notebook, we explore how to manage **duplicates** at query time and retain the **latest updates** with the help of Apache Spark’s **window functions**. These powerful functions enable you to:\n",
    "\n",
    "- Partition the data based on a unique identifier (`id`).\n",
    "- Order the records by a timestamp (or another field to determine the most recent entry).\n",
    "- Apply row numbering to isolate and filter the latest record.\n",
    "\n",
    "By leveraging this approach, we can efficiently handle large datasets while ensuring that only the most up-to-date information is processed.\n",
    "\n",
    "### 1. Setup Spark Session\n",
    "\n",
    "To begin with, we initialize a Spark session, which will allow us to interact with Spark using DataFrames and SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047cc0e1-2663-4870-81a6-c1a46f14b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UpsertExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442a3be-dafd-4ba2-9a32-1fd7bd9cae94",
   "metadata": {},
   "source": [
    "### 2. Sample Data\n",
    "\n",
    "Let’s create some sample data that simulates a scenario where records with the same `id` are inserted multiple times, but with different timestamps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7f5bbd-35bf-495b-91eb-046bb36d8661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------------+\n",
      "|id |name         |timestamp          |\n",
      "+---+-------------+-------------------+\n",
      "|1  |Alice        |2024-11-18 10:00:00|\n",
      "|2  |Bob          |2024-11-18 11:00:00|\n",
      "|1  |Alice Updated|2024-11-18 12:00:00|\n",
      "|3  |Charlie      |2024-11-18 09:00:00|\n",
      "|2  |Bob Updated  |2024-11-18 12:00:00|\n",
      "+---+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data with possible duplicates\n",
    "data = [\n",
    "    Row(id=1, name=\"Alice\", timestamp=\"2024-11-18 10:00:00\"),\n",
    "    Row(id=2, name=\"Bob\", timestamp=\"2024-11-18 11:00:00\"),\n",
    "    Row(id=1, name=\"Alice Updated\", timestamp=\"2024-11-18 12:00:00\"),  # Duplicate ID with later timestamp\n",
    "    Row(id=3, name=\"Charlie\", timestamp=\"2024-11-18 09:00:00\"),\n",
    "    Row(id=2, name=\"Bob Updated\", timestamp=\"2024-11-18 12:00:00\")  # Duplicate ID with later timestamp\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741dd1d-9d50-4f66-aadc-a5cf2abb7c27",
   "metadata": {},
   "source": [
    "### 3. Remove Duplicates and Keep the Latest Record\n",
    "\n",
    "Next, we’ll use a **window function** to remove duplicates and keep only the **latest record** for each `id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b837d4db-18c5-49b3-8c1b-1c023719a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------------+\n",
      "|id |name         |timestamp          |\n",
      "+---+-------------+-------------------+\n",
      "|1  |Alice Updated|2024-11-18 12:00:00|\n",
      "|2  |Bob Updated  |2024-11-18 12:00:00|\n",
      "|3  |Charlie      |2024-11-18 09:00:00|\n",
      "+---+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window specification to partition by 'id' and order by 'timestamp' descending (latest first)\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(col(\"timestamp\").desc())\n",
    "\n",
    "# Add a row number column to select the latest record for each 'id'\n",
    "df_with_rownum = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Filter the rows where row_num = 1, which gives us the latest record for each id\n",
    "latest_df = df_with_rownum.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Show the result\n",
    "latest_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652330ff-aa8a-48bc-b8c5-dcfce67034f8",
   "metadata": {},
   "source": [
    "### 4. Why is the Window Function Required?\n",
    "\n",
    "The **window function** is crucial for handling this scenario. Here's why:\n",
    "\n",
    "#### a) Partitioning Data\n",
    "\n",
    "The `Window.partitionBy(\"id\")` part of the window function groups the data by the `id` column. This allows us to handle each `id` independently and apply operations (like sorting by timestamp) to only the records with the same `id`.\n",
    "\n",
    "#### b) Sorting Within Partitions\n",
    "\n",
    "The `orderBy(col(\"timestamp\").desc())` ensures that records within each partition (grouped by `id`) are ordered by `timestamp` in **descending order**. This sorting is important because we need to select the most recent (latest) record for each `id`.\n",
    "\n",
    "#### c) Row Number Assignment\n",
    "\n",
    "By using `row_number().over(window_spec)`, we assign a row number to each record in the partition. This row number reflects the position of each record based on the descending timestamp order. The most recent record for each `id` will receive the row number 1.\n",
    "\n",
    "#### d) Filtering for the Latest Record\n",
    "\n",
    "After assigning the row numbers, we filter the rows where `row_num == 1`. This ensures that we keep only the latest record for each `id`, effectively removing any earlier duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e9e1a",
   "metadata": {},
   "source": [
    "## Optimizing Data Processing with Filters\n",
    "\n",
    "Reading the entire dataset is usually not practical or necessary, especially when dealing with large-scale data. In real-world scenarios, it's common to apply filters to narrow down the scope of data processing. For instance:\n",
    "\n",
    "1. **Filter by Time Window**:  \n",
    "   You can include a filter to process only data within a specific time range. This is particularly useful for incremental processing or handling recent changes:\n",
    "   ```python\n",
    "   filtered_df = df.filter((col(\"timestamp\") >= \"2024-01-01\") & (col(\"timestamp\") <= \"2024-01-31\"))\n",
    "   ```\n",
    "\n",
    "   E.g.\n",
    "\n",
    "   ```python\n",
    "   filtered_df = df.filter((col(\"timestamp\") >= \"2024-01-01\") & (col(\"timestamp\") <= \"2024-01-31\"))\n",
    "   window_spec = Window.partitionBy(\"id\").orderBy(col(\"timestamp\").desc())\n",
    "   df_with_rownum = filtered_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "   ```\n",
    "\n",
    "2. **Filter by Entity**:  \n",
    "   When working with data tied to a specific entity (e.g., a user, customer, or region), you can filter by the relevant entity ID or group:\n",
    "   ```python\n",
    "   filtered_df = df.filter(col(\"entity_id\") == \"12345\")\n",
    "   ```\n",
    "\n",
    "   E.g.\n",
    "\n",
    "   ```python\n",
    "   filtered_df = df.filter(col(\"entity_id\") == \"12345\")\n",
    "   window_spec = Window.partitionBy(\"id\").orderBy(col(\"timestamp\").desc())\n",
    "   df_with_rownum = filtered_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "   ```\n",
    "\n",
    "By applying these filters before leveraging window functions, you can optimize performance, reduce resource usage, and focus on the subset of data that matters most for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4c0f0-96c8-447e-8b23-cdcaae941af2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to handle **duplicates** and keep the **latest updates** using Apache Spark’s **window functions**. The window function allows you to:\n",
    "- Partition data by a unique identifier (`id`).\n",
    "- Sort the records by a timestamp (or any other field to determine the latest).\n",
    "- Use row numbering to identify and filter out the most recent record.\n",
    "\n",
    "This method is highly scalable and efficient, especially when working with large datasets, and ensures that only the most recent data is processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb5d2e-10e7-445b-892a-42d269bd8346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
