{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b7d9e4-b2e4-417c-afaf-131334eb1484",
   "metadata": {},
   "source": [
    "# Spark Query Time Merge (non-id)\n",
    "\n",
    "## Handling Duplicates and Latest Updates with Apache Spark\n",
    "\n",
    "```{seealso}\n",
    "[Query Merge vs Insert Merge](./query_time_merge.md)\n",
    "```\n",
    "\n",
    "In this notebook, we explore how to manage **duplicates** at query time and retain the **latest updates** with the help of Apache Spark’s **window functions**. These powerful functions enable you to:\n",
    "\n",
    "- Partition the data based on one or more key fields (e.g., a unique identifier or combinations like `first_name` and `last_name`).\n",
    "- Order the records by a timestamp (or any other field) to determine the most recent entry.\n",
    "- Apply row numbering to isolate and filter the latest record for each unique combination of key fields.\n",
    "\n",
    "By leveraging this approach, we can efficiently handle large datasets, even when records are inserted only or identified by non-ID fields, while ensuring that only the most up-to-date information is processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c40730-2b9a-483c-adc8-a463d7cee22a",
   "metadata": {},
   "source": [
    "### Handling Multiple Non-ID Fields (e.g., `first_name` and `last_name`)\n",
    "\n",
    "In cases where records are identified by a combination of multiple fields (like `first_name` and `last_name`), the window function can be adjusted to partition by both of these fields. This approach will allow us to select the latest record for each unique combination of `first_name` and `last_name`.\n",
    "\n",
    "#### 1. Setup Spark Session\n",
    "\n",
    "To begin with, we initialize a Spark session, which will allow us to interact with Spark using DataFrames and SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2669c49-f754-4e1f-809c-421ec01ab2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UpsertExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d8946-f697-41a8-8a55-fcfeb02e0678",
   "metadata": {},
   "source": [
    "#### 2. Sample Data\n",
    "\n",
    "We’ll create data where records are identified by both `first_name` and `last_name`, and some records may have duplicate combinations with different timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce523600-6e22-48df-8afd-cd5f706f47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+\n",
      "|first_name|last_name|timestamp          |\n",
      "+----------+---------+-------------------+\n",
      "|Alice     |Smith    |2024-11-18 10:00:00|\n",
      "|Bob       |Johnson  |2024-11-18 11:00:00|\n",
      "|Alice     |Smith    |2024-11-18 12:00:00|\n",
      "|Charlie   |Brown    |2024-11-18 09:00:00|\n",
      "|Bob       |Johnson  |2024-11-18 12:00:00|\n",
      "+----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data with possible duplicates based on first_name and last_name\n",
    "data = [\n",
    "    Row(first_name=\"Alice\", last_name=\"Smith\", timestamp=\"2024-11-18 10:00:00\"),\n",
    "    Row(first_name=\"Bob\", last_name=\"Johnson\", timestamp=\"2024-11-18 11:00:00\"),\n",
    "    Row(first_name=\"Alice\", last_name=\"Smith\", timestamp=\"2024-11-18 12:00:00\"),  # Duplicate with later timestamp\n",
    "    Row(first_name=\"Charlie\", last_name=\"Brown\", timestamp=\"2024-11-18 09:00:00\"),\n",
    "    Row(first_name=\"Bob\", last_name=\"Johnson\", timestamp=\"2024-11-18 12:00:00\")  # Duplicate with later timestamp\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e22c4-bb70-4a12-b869-c85133efc4ca",
   "metadata": {},
   "source": [
    "#### 3. Window Function with Multiple Partition Keys\n",
    "\n",
    "Now, instead of partitioning by `id`, we will partition by both `first_name` and `last_name`. We will still sort by `timestamp` in descending order to pick the latest record for each unique combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "569f5945-0954-43a5-a65e-3a7391953d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+\n",
      "|first_name|last_name|timestamp          |\n",
      "+----------+---------+-------------------+\n",
      "|Alice     |Smith    |2024-11-18 12:00:00|\n",
      "|Bob       |Johnson  |2024-11-18 12:00:00|\n",
      "|Charlie   |Brown    |2024-11-18 09:00:00|\n",
      "+----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window specification to partition by 'first_name' and 'last_name' and order by 'timestamp' descending\n",
    "window_spec = Window.partitionBy(\"first_name\", \"last_name\").orderBy(col(\"timestamp\").desc())\n",
    "\n",
    "# Add a row number column to select the latest record for each (first_name, last_name) combination\n",
    "df_with_rownum = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Filter the rows where row_num = 1, which gives us the latest record for each combination\n",
    "latest_df = df_with_rownum.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Show the result\n",
    "latest_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d34cc-be30-4126-a7d9-9157190fa0f7",
   "metadata": {},
   "source": [
    "#### 4. Why Use Multiple Partition Keys?\n",
    "\n",
    "When dealing with **multiple non-ID fields** (such as `first_name` and `last_name`), the logic changes slightly, but the concept remains the same. The window function allows us to:\n",
    "\n",
    "1. **Partition by Multiple Fields**:\n",
    "   - `Window.partitionBy(\"first_name\", \"last_name\")` groups the data by both `first_name` and `last_name`, effectively treating this combination as a unique key.\n",
    "   - This ensures that we consider records with the same `first_name` and `last_name` as duplicates, even if their `id` is not available.\n",
    "   \n",
    "2. **Sort Within Each Partition**:\n",
    "   - We use `orderBy(col(\"timestamp\").desc())` to sort records within each partition (group of the same `first_name` and `last_name`) by `timestamp` in descending order. This ensures that the most recent record comes first.\n",
    "\n",
    "3. **Assign Row Numbers**:\n",
    "   - `row_number().over(window_spec)` assigns a row number to each record within its partition. The most recent record will have the row number `1` because we sorted by the timestamp in descending order.\n",
    "\n",
    "4. **Filter for Latest Record**:\n",
    "   - By filtering `row_num == 1`, we select only the latest record from each partition, ensuring that only the most recent entry for each unique combination of `first_name` and `last_name` is retained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e80d4b",
   "metadata": {},
   "source": [
    "## Optimizing Data Processing with Filters\n",
    "\n",
    "Reading the entire dataset is usually not practical or necessary, especially when dealing with large-scale data. In real-world scenarios, it's common to apply filters to narrow down the scope of data processing. For instance:\n",
    "\n",
    "1. **Filter by Time Window**:  \n",
    "   You can include a filter to process only data within a specific time range. This is particularly useful for incremental processing or handling recent changes:\n",
    "   ```python\n",
    "   filtered_df = df.filter((col(\"timestamp\") >= \"2024-01-01\") & (col(\"timestamp\") <= \"2024-01-31\"))\n",
    "   ```\n",
    "\n",
    "   E.g.\n",
    "\n",
    "   ```python\n",
    "   filtered_df = df.filter((col(\"timestamp\") >= \"2024-01-01\") & (col(\"timestamp\") <= \"2024-01-31\"))\n",
    "   window_spec = Window.partitionBy(\"id\").orderBy(col(\"timestamp\").desc())\n",
    "   df_with_rownum = filtered_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "   ```\n",
    "\n",
    "2. **Filter by Entity**:  \n",
    "   When working with data tied to a specific entity (e.g., a user, customer, or region), you can filter by the relevant entity ID or group:\n",
    "   ```python\n",
    "   filtered_df = df.filter(col(\"entity_id\") == \"12345\")\n",
    "   ```\n",
    "\n",
    "   E.g.\n",
    "\n",
    "   ```python\n",
    "   filtered_df = df.filter(col(\"entity_id\") == \"12345\")\n",
    "   window_spec = Window.partitionBy(\"id\").orderBy(col(\"timestamp\").desc())\n",
    "   df_with_rownum = filtered_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "   ```\n",
    "\n",
    "By applying these filters before leveraging window functions, you can optimize performance, reduce resource usage, and focus on the subset of data that matters most for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c6475-49e3-485a-957f-6429eedf3f1e",
   "metadata": {},
   "source": [
    "### Conclusion: Handling Multiple Fields in Upsert Logic\n",
    "\n",
    "In this updated example, we demonstrated how to handle multiple non-ID fields (`first_name` and `last_name`) as keys to identify records. The window function's flexibility allows you to partition by any combination of fields, not just a single identifier, making it suitable for scenarios where records might not have a unique ID but can be identified by other fields.\n",
    "\n",
    "By using partitioning, sorting, and row numbering, we can efficiently deduplicate and ensure only the latest records are kept, regardless of whether the data has a unique identifier like `id` or is based on other attributes like names.\n",
    "\n",
    "Would you like to explore additional use cases or need further explanations about the Spark functions used here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2f6dc-794a-4d56-b188-5e507bedf35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
